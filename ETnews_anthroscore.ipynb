{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingchong/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese-roberta model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gc\n",
    "import os  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"chinese-roberta model loaded on %s\"%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target terms\n",
    "terms = ['他', '她', '它']\n",
    "\n",
    "def get_prediction(original_sent, masked_sent):\n",
    "    if '[MASK]' not in masked_sent:\n",
    "        return np.zeros((len(terms),))\n",
    "\n",
    "    # Get the IDs of target terms in the vocabulary\n",
    "    target_inds = [tokenizer.get_vocab().get(x, -1) for x in terms]\n",
    "    if -1 in target_inds:\n",
    "        raise ValueError(\"Some terms are not in the tokenizer vocabulary!\")\n",
    "\n",
    "    # **Step 1: Expose RoBERTa to the original sentence**\n",
    "    with torch.no_grad():\n",
    "        original_input_ids = tokenizer.encode(original_sent, return_tensors='pt').to(device)\n",
    "        model(original_input_ids)  # RoBERTa processes full context but doesn't return output\n",
    "\n",
    "    # **Step 2: Predict on the masked sentence**\n",
    "    masked_input_ids = tokenizer.encode(masked_sent, return_tensors='pt').to(device)\n",
    "\n",
    "    # Find the position of the [MASK] token\n",
    "    masked_position = (masked_input_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "\n",
    "    if not masked_position.size(0):\n",
    "        return np.zeros((len(terms),))\n",
    "\n",
    "    predictions = torch.zeros(masked_position.size(0), len(terms))\n",
    "\n",
    "    for i in range(masked_position.size(0)):\n",
    "        masked_pos = masked_position[i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(masked_input_ids)  # Get predictions for masked sentence\n",
    "\n",
    "        # Extract the logits of the [MASK] token\n",
    "        last_hidden_state = output.logits[0, masked_pos].squeeze()\n",
    "\n",
    "        # Compute softmax over the vocabulary\n",
    "        lhs_softmax = torch.softmax(last_hidden_state, dim=0)\n",
    "\n",
    "        for j, term_index in enumerate(target_inds):\n",
    "            predictions[i, j] = lhs_softmax[term_index]\n",
    "\n",
    "    return predictions[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentences_from_file(input_filename, entities_file, text_column_name, id_column_name, output_filename, total_sentences_filename):\n",
    "    with open(entities_file, 'r', encoding='utf-8') as f:\n",
    "        entities = sorted([line.strip() for line in f.readlines()], key=len, reverse=True)\n",
    "\n",
    "    df = pd.read_csv(input_filename)\n",
    "\n",
    "    results = []\n",
    "    total_sentences_data = []  \n",
    "    total_sentences = 0  \n",
    "\n",
    "    def custom_sentence_split(text):\n",
    "        sentences = []\n",
    "        pattern = r'([^。\\n]*[。]?)(?:」)?(?=\\n|$|[^」])|([^！\\n]*[！]?)(?:」)?(?=\\n|$|[^」])'   \n",
    "        matches = list(re.finditer(pattern, text))\n",
    "\n",
    "        last_end = 0\n",
    "        for match in matches:\n",
    "            sentence = match.group(0)\n",
    "            sentences.append(sentence)\n",
    "            last_end = match.end()\n",
    "\n",
    "        if last_end < len(text):\n",
    "            sentences.append(text[last_end:])\n",
    "\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[text_column_name]\n",
    "        text_id = row[id_column_name]\n",
    "\n",
    "        sentences = custom_sentence_split(text)\n",
    "        total_sentences += len(sentences) \n",
    "        total_sentences_data.extend([{\"text_id\": text_id, \"sentence\": s} for s in sentences])\n",
    "\n",
    "        for sentence in sentences:\n",
    "            original_sentence = sentence  \n",
    "            text_id = row[id_column_name]\n",
    "            positions = []\n",
    "\n",
    "            for entity in entities:\n",
    "                if re.search(r'[\\u4e00-\\u9fff]', entity):  # For Chinese terms\n",
    "                    pattern = r'(' + re.escape(entity) + r')'\n",
    "                else:  # For English Terms\n",
    "                    pattern = r'(?<![a-zA-Z0-9.])(' + re.escape(entity) + r')(?![a-zA-Z0-9.])'\n",
    "\n",
    "                matches = list(re.finditer(pattern, sentence))\n",
    "                for match in matches:\n",
    "                    positions.append((match.start(), match.end(), entity))\n",
    "\n",
    "            # Sort by the position of entity occurrences\n",
    "            positions.sort(key=lambda x: x[0])\n",
    "\n",
    "            # Mask based on the original sentence, masking only one entity at a time\n",
    "            for pos in positions:\n",
    "                start, end, entity = pos\n",
    "                masked_sentence = (\n",
    "                    original_sentence[:start] +\n",
    "                    \"[MASK]\" +\n",
    "                    original_sentence[end:]\n",
    "                )\n",
    "                results.append({\n",
    "                    \"sentence\": original_sentence,  \n",
    "                    \"masked_sentence\": masked_sentence,\n",
    "                    \"text_id\": text_id,\n",
    "                    \"original_term\": entity\n",
    "                })\n",
    "\n",
    "    print(f\"Total sentences parsed: {total_sentences}\")\n",
    "\n",
    "    # Create a DataFrame for all sentences\n",
    "    total_sentences_df = pd.DataFrame(total_sentences_data)\n",
    "    total_sentences_df.to_csv(total_sentences_filename, index=False, encoding='utf-8')\n",
    "    print(f\"Total sentences saved to {total_sentences_filename}\")\n",
    "\n",
    "    # Create a DataFrame for sentences containing entities\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    print(f\"{len(output_df)} sentences containing entities processed and saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sentences_from_file(\n",
    "    input_filename=\"Dataset_filename\",\n",
    "    entities_file=\"entities.txt\",\n",
    "    text_column_name=\"content\",\n",
    "    id_column_name=\"id\",\n",
    "    output_filename=\"Output_filename\",\n",
    "    total_sentences_filename=\"Total_output_filename\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.7f}'.format  \n",
    "\n",
    "def get_probs(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    terms = ['他', '她', '它']\n",
    "\n",
    "    # Initialize columns for each term with 0.0\n",
    "    for term in terms:\n",
    "        df[term] = 0.0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Let RoBERTa see the complete sentence first, then predict the MASKed sentence\n",
    "            scores = get_prediction(row['sentence'], row['masked_sentence'])\n",
    "\n",
    "            # If the length of scores matches the length of terms, update the DataFrame\n",
    "            if len(scores) == len(terms):\n",
    "                for i, term in enumerate(terms):\n",
    "                    df.at[index, term] = scores[i]\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected scores length for sentence '{row['masked_sentence']}'\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence '{row['masked_sentence']}': {e}\")\n",
    "            continue\n",
    "\n",
    "    output_filename = 'updated_' + filename\n",
    "    output_dir = os.path.dirname(output_filename)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    df.to_csv(output_filename, float_format='%.7f', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_probs('Output_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicates\n",
    "raw_df = pd.read_csv('updated_Output_filename')\n",
    "original_len = len(raw_df)\n",
    "duplicate_rows = raw_df[raw_df['masked_sentence'].duplicated(keep=False)]\n",
    "selected_columns = duplicate_rows[['masked_sentence', 'text_id']]\n",
    "\n",
    "raw_df_unique = raw_df.drop_duplicates(subset='masked_sentence', keep='first')\n",
    "unique_len = len(raw_df_unique)\n",
    "print(\"Number of unique entries after dropping duplicates:\", unique_len)\n",
    "dropped_rows = original_len - unique_len\n",
    "print(\"Number of dropped rows:\", dropped_rows)\n",
    "raw_df_unique.to_csv('unique_dataset', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold filtering\n",
    "\n",
    "df = pd.read_csv('updated_Output_filename')\n",
    "original_len = len(df) \n",
    "print(f'原始資料筆數: {original_len}')\n",
    "\n",
    "columns_to_check = ['他', '她', '它']\n",
    "thresh = 1 / 21128\n",
    "\n",
    "def filter_any_column_in_range(df, columns, ranges):\n",
    "    # Initialize condition to False (not matching)\n",
    "    condition = pd.Series(False, index=df.index)\n",
    "\n",
    "    for min_val, max_val in ranges:\n",
    "        for col in columns:\n",
    "            # Update condition: True if any column matches the range\n",
    "            condition |= (df[col] >= min_val) & (df[col] <= max_val)\n",
    "\n",
    "    return df[condition]\n",
    "\n",
    "# Store the previous counts for comparison\n",
    "previous_counts = {col: original_len for col in columns_to_check}  # Initialize to the original length\n",
    "\n",
    "for factor in range(1, 31):\n",
    "    ranges_to_check = [(factor * thresh, 1.0)]\n",
    "\n",
    "    filtered_df = filter_any_column_in_range(df, columns_to_check, ranges_to_check)\n",
    "\n",
    "    print(f'倍數 * {factor}')\n",
    "    print(f'篩選後的筆數: {len(filtered_df)}')\n",
    "\n",
    "    for col in columns_to_check:\n",
    "        col_condition = (filtered_df[col] >= (factor * thresh)) & (filtered_df[col] <= 1.0)\n",
    "        current_count = col_condition.sum()\n",
    "\n",
    "        # Calculate how many rows were deducted compared to the previous round\n",
    "        rows_deducted = previous_counts[col] - current_count\n",
    "\n",
    "        # For the first round, do not show \"減\" and only show the count\n",
    "        if factor == 1:\n",
    "            print(f'{col} 符合條件的筆數: {current_count}')\n",
    "        else:\n",
    "            print(f'{col} 符合條件的筆數: {current_count} (減 {rows_deducted} 筆) ({round(rows_deducted/original_len * 100,4)}%)')\n",
    "\n",
    "        previous_counts[col] = current_count\n",
    "\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving factor 14 data\n",
    "round_to_save = 14\n",
    "\n",
    "for factor in range(1, 31):\n",
    "    ranges_to_check = [(factor * thresh, 1.0)]\n",
    "    \n",
    "    filtered_df = filter_any_column_in_range(df, columns_to_check, ranges_to_check)\n",
    "\n",
    "    if factor == round_to_save:\n",
    "        filtered_df.to_csv(f'f{factor}dataset.csv', index=False)\n",
    "        print(f'File saved as round_{factor}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anthroscores A(sx)\n",
    "\n",
    "def get_anthroscores(sentence_filename):\n",
    "    df = pd.read_csv(sentence_filename)\n",
    "    df['human_scores'] = df['他'] + df['她']\n",
    "    df['nonhuman_scores'] = df['它']\n",
    "    df['human_scores'] = np.where(df['human_scores'] > 0, df['human_scores'], 1e-8)\n",
    "    df['nonhuman_scores'] = np.where(df['nonhuman_scores'] > 0, df['nonhuman_scores'], 1e-8)\n",
    "\n",
    "    df['anthroscore'] = np.log(df['human_scores']) - np.log(df['nonhuman_scores'])\n",
    "    \n",
    "    for col in ['他', '她', '它', 'human_scores', 'nonhuman_scores']:\n",
    "        df[col] = df[col].apply(lambda x: f'{x:.6f}')\n",
    "\n",
    "    output_filename = os.path.join('updated_Dataset', 'anthro_' + os.path.basename(sentence_filename))    \n",
    "    df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_anthroscores('14dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Average Score\n",
    "def compute_average_scores(input_file, output_file, text_id_name):\n",
    "    if input_file.endswith('csv'):\n",
    "        original_df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Calculate average scores without modifying original_df\n",
    "    unique_df = original_df.copy()\n",
    "    id_counts = unique_df[text_id_name].value_counts()\n",
    "    \n",
    "    # Add Average_anthroscore column only to unique_df\n",
    "    unique_df['Average_anthroscore'] = unique_df['anthroscore']\n",
    "    duplicate_ids = id_counts[id_counts > 1].index\n",
    "    \n",
    "    for text_id in duplicate_ids:\n",
    "        mask = unique_df[text_id_name] == text_id\n",
    "        total_sentences = id_counts[text_id]\n",
    "        sum_scores = unique_df.loc[mask, 'anthroscore'].sum()\n",
    "        avg_score = sum_scores / total_sentences\n",
    "        unique_df.loc[mask, 'Average_anthroscore'] = avg_score\n",
    "    \n",
    "    # Keep only one row per text_id\n",
    "    unique_df = unique_df.drop_duplicates(subset=[text_id_name]).copy()\n",
    "    unique_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "input_file = 'anthro_f14_dataset08.csv'\n",
    "output_file = 'avg_anthro_f14_dataset08.csv'\n",
    "text_id_name = 'text_id'  \n",
    "unique_df = compute_average_scores(input_file, output_file, text_id_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
